# Special handling for slow AWS uploads with memory constrained tables
spark.network.timeout   600s
spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.WebIdentityTokenCredentialsProvider
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
# setting this to false per https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions
spark.sql.adaptive.coalescePartitions.parallelismFirst false
# https://github.com/apache/spark/pull/32518#issuecomment-1058840240
# https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html
# https://spot.io/blog/improve-apache-spark-performance-with-the-s3-magic-committer/
spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled true
spark.hadoop.fs.s3a.directory.marker.retention keep
spark.hadoop.parquet.enable.summary-metadata false
spark.sql.parquet.mergeSchema false
spark.sql.parquet.filterPushdown true
spark.sql.parquet.int96AsTimestamp true
spark.sql.parquet.int96RebaseModeInWrite CORRECTED
spark.sql.parquet.int96RebaseModeInRead CORRECTED
spark.sql.parquet.datetimeRebaseModeInRead CORRECTED
spark.sql.parquet.datetimeRebaseModeInWrite CORRECTED
spark.sql.hive.metastorePartitionPruning true
# spark.sql.execution.arrow.pyspark.enabled is set to true to enable Arrow optimization for PySpark
spark.sql.execution.arrow.pyspark.enabled true
# spark.sql.execution.arrow.pyspark.fallback.enabled is set to false to disable Arrow optimization fallback for PySpark
spark.sql.execution.arrow.pyspark.fallback.enabled false
