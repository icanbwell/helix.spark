# Special handling for slow AWS uploads with memory constrained tables
spark.network.timeout   600s
spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.WebIdentityTokenCredentialsProvider
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
# setting this to false per https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions
spark.sql.adaptive.coalescePartitions.parallelismFirst false
# https://spot.io/blog/improve-apache-spark-performance-with-the-s3-magic-committer/
spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled true
spark.hadoop.fs.s3a.directory.marker.retention keep
# spark.sql.execution.arrow.pyspark.enabled is set to true to enable Arrow optimization for PySpark
spark.sql.execution.arrow.pyspark.enabled true
# spark.sql.execution.arrow.pyspark.fallback.enabled is set to false to disable Arrow optimization fallback for PySpark
spark.sql.execution.arrow.pyspark.fallback.enabled false
